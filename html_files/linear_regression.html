<!DOCTYPE html>
<html lang="en-us">

<head>
  <link rel="icon" href="../Images/House_icon.png"
  <meta charset="UTF-8">
  <title>California Housing Machine Learning</title>

 <!-- Bring in our bootstrap stylesheet -->

 
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

 <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
 <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
 <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

 <link rel="stylesheet" href="style.css">
</head>

<body>
    <nav class="navbar navbar-expand-lg navbar-dark" style="background-color: rgb(4, 82, 39)">
      <img src="../Images/california-flag-acegif-1.gif" height="45" width="55">
        <a class="navbar-brand" href="../index.html">ML California Housing</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarColor01" aria-controls="navbarColor01" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>      
      
        <div class="collapse navbar-collapse" id="navbarColor01">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item active">
              <a class="nav-link" href="../index.html"> Home <span class="sr-only">(current)</span></a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="data.html">Data</a>
              </li>
            <li class="nav-item dropdown">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  ML Models
                </a>
                <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="logistic.html">Logistic Regression </a>
                  <a class="dropdown-item" href="linear_regression.html">Linear Regression</a>
                  
                </div>
              </li>
          </ul>
          </div>
        </nav>
  <div class="container">


  
      <!-- Row 1 -->
      <div class="row">
        <div class="col-12 col-sm-12 col-md-12">
            <div class="jumbotron" style="background-color: rgb(253, 221, 221)">
              <h1><strong>Multiple Linear Regression</strong></h1>
              
              <p><strong>Base Linear Regression Model</strong></p>
              <p>The Base Model serves as our starting point for evaluating the various model optimization methods. The only preprocessing steps taken on our data for the base model were label encoding
                categorical variables (Ocean Proximity) and using the Standard Scaler. The default hyperparameters were used for Train_Test_Split and Linear Regression.</p>
              
              <center><table>
                <th colspan="2">Linear Regression</th>
                <tr>
                  <td><center>Training Score</center></td>
                  <td>0.6387324933349059</td>
                </tr>
                <tr>
                  <td><center>Testing Score</center></td>
                  <td>0.653698716213917</td>
                </tr>
              </table></center>
              <!-- <center> <img src="../Images/basemodelresidualplot.png" height="300" width="400"></center> -->
              <p></p>
              <p>The R-Squared of 65% serves as our starting point to which we will attempt to further optimize the model. Interestingly, our model actually performs better on the 
                testing data, than training data. If we were to feed our model more data or manipulate the split, however; we would not expect this to continue.
              </p>

              <p><strong>Lasso Regression Model</strong></p>
              <p>Our first attempt at optimization involves using a Lasso Regression. Due to the high number of features and multicollinearity with our data, the Lasso Regression would provide
                automated feature selection which we expect to improve accuracy.
              </p>
              
              <center><table>
                <th colspan="2">Lasso</th>
                <tr>
                  <td><center>Training Score</center></td>
                  <td>0.6387324859582684</td>
                </tr>
                <tr>
                  <td><center>Testing Score</center></td>
                  <td>0.6536976224112978</td>
                </tr>
              </table></center>
              <!-- <center> <img src="../Images/lassoresidualplot.png" height="300" width="400"></center> -->
              <p></p>
              <p>The Lasso Regression ultimately gives nearly identical results to the Linear Regression...
              </p>
              <p><strong>Ridge Regression Model</strong></p>
              <p>Our next attempt at optimization involves using a Ridge Regression. This will function similarly to the Lasso Regression by applying a penalty factor, however; Ridge will not set coefficients
                to zero, while Lasso will.
              </p>
              
              <center><table>
                <th colspan="2">Ridge</th>
                <tr>
                  <td><center>Training Score</center></td>
                  <td>0.638732387055294</td>
                </tr>
                <tr>
                  <td><center>Testing Score</center></td>
                  <td>0.6536977532921333</td>
                </tr>
              </table></center>
              <!-- <center> <img src="../Images/ridgeresidualplot.png" height="300" width="400"></center> -->
              <p></p>
              <p>Again, the Ridge Regression ultimately gives nearly identical results to the Linear Regression and Lasso Regression, performing just slightly better.
              </p>
              <p><strong>Elastic Net Regression Model</strong></p>
              <p>Our final attempt at optimizing using a regularization technique is by using Elastic Net. This uses both the regularization of Ridge and Lasso. 
              </p>
              
              <center><table>
                <th colspan="2">Elastic Net</th>
                <tr>
                  <td><center>Training Score</center></td>
                  <td>0.5477983685738015</td>
                </tr>
                <tr>
                  <td><center>Testing Score</center></td>
                  <td>0.5590469059783605
                  </td>
                </tr>
              </table></center>
              <!-- <center> <img src="../Images/elasticnetresidualplot.png" height="300" width="400"></center> -->
              <p></p>
              <p>This ultimately gives us worse performance than any of the previous models.
              </p>
              <p><strong>Random Forest Regressor Model</strong></p>
              <p>For our final attempt at optimization, we use a Random Forest Regressor. With limited success using regularization to improve our model's accuracy, 
                Random Forest can potentially improve through building multiple decision trees. Our first attempt using the Random Forest Regressor involves no changes to default hyperparameters.
              </p>
              
              <center><table>
                <th colspan="2">Random Forest Regressor</th>
                <tr>
                  <td><center>Training Score</center></td>
                  <td>0.9743170985213362</td>
                </tr>
                <tr>
                  <td><center>Testing Score</center></td>
                  <td>0.8316238857990677
                  </td>
                </tr>
              </table></center>
              <!-- <center> <img src="../Images/elasticnetresidualplot.png" height="300" width="400"></center> -->
              <p></p>
              <p>Based on our training score significantly outperforming our testing score, we conclude our model is overfit. This overfitting is because
                we do not have enough data and the model has become too specific with each decision tree being too deep. To correct this, we tune the max_depth hyperparameter.
              We will start by setting a max_depth = 5.</p>
              
              <center><table>
                <th colspan="2">Random Forest Regressor (max_depth = 5)</th>
                <tr>
                  <td><center>Training Score</center></td>
                  <td>0.6587947238892986</td>
                </tr>
                <tr>
                  <td><center>Testing Score</center></td>
                  <td>0.6623595251381666
                  </td>
                </tr>
              </table></center>
              <p>With the adjustment to max_depth, this solves our overfitting problem, but reduces the model's accuracy. To attempt to balance overfitting and model accuracy 
                we will now set max_depth = 7
              </p>
              <center><table>
                <th colspan="2">Random Forest Regressor (max_depth = 7)</th>
                <tr>
                  <td><center>Training Score</center></td>
                  <td>0.7505508565310899</td>
                </tr>
                <tr>
                  <td><center>Testing Score</center></td>
                  <td>0.7345784196197869
                  </td>
                </tr>
              </table></center>

              

        </div>
    </div>
      </div>
      </div>
</body>
</html>
