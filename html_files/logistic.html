<!DOCTYPE html>
<html lang="en-us">

<head>
  <link rel="icon" href="../Images/House_icon.png"
  <meta charset="UTF-8">
  <title>California Housing Machine Learning</title>

 <!-- Bring in our bootstrap stylesheet -->

 
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

 <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
 <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
 <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

 <link rel="stylesheet" href="style.css">
</head>

<body>
  <nav class="navbar fixed-top navbar-expand-lg navbar-dark">
    <img src="../Images/california-flag-acegif-1.gif" height="45" width="55">
      <a class="navbar-brand" href="../index.html">ML California Housing</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarColor01" aria-controls="navbarColor01" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>      

      <div class="collapse navbar-collapse" id="navbarColor01">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item active">
            <a class="nav-link" href="../index.html"> Home <span class="sr-only">(current)</span></a>
          </li>
          <li class="nav-item">
              <a class="nav-link" href="data.html">Data</a>
            </li>
          <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                ML Models
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                <a class="dropdown-item" href="logistic.html">Logistic Regression </a>
                <a class="dropdown-item" href="linear_regression.html">Linear Regression</a>
                
              </div>
            </li>
            <div class="custom-control custom-switch mr-5">
              <input type="checkbox" class="custom-control-input" id="selector">
              <label class="custom-control-label" for="selector">Dark Mode</label>
            </div>
        </ul>
        </div>
      </nav>
<div class="container">


  
      <!-- Row 1 -->
      <div class="row">
        <div class="col-12 col-sm-12 col-md-12">
            <div class="jumbotron">
              <h1><strong>Logistic Regression Algorithm</strong></h1>
              

              <p> One of the regression models we have decided to use for our Project was Multinomial logistic regression where we have assigned three classifications to the Median Housing Value, categorizing the outputs into three different ranges as shown in Table 1 below.</p>
              <center><table>
                <p></p>  
                <th>Classification</th>
                <th>Bins</th>
                <tr>
                  <td><center>0</center></td>
                  <td>Median House Value greater than $400,000</td>
                </tr>
                <tr>
                  <td><center>1</center></td>
                  <td>Median House Value between $250,000 and $400,000</td>
                </tr>
                <tr>
                    <td><center>2</center></td>
                    <td>Median House Value less than $250,000</td>
                  </tr>
                
              </table></center>
              <p><strong>Table 1.</strong> Classifications of Median Housing Value in California. From the 1990 Census dataset.</p>
              <p></p>
              <p>With the deployment of the Logistic Regression, we were able to not only predict the range of the median house value by looking at different Housing dimensions but also attain a precision of predictions of each range of the value and assess at what ranges our model had the best precision in predicting the median house value. </p>

              <p><strong>Classification Report of logistic regression</strong></p>
              <p> </p>
              
              <center><img src="../Images/Log_Reg_training_testing_score.PNG" alt="LR image1" height="300" width="600"></center>
              </p>
              <center><table>
                <th colspan="2">Base Model</th>
                <tr>
                  <td><center>Training Score</center></td>
                  <td>0.8091258188389429</td>
                </tr>
                <tr>
                  <td><center>Testing Score</center></td>
                  <td>0.8145973154362416</td>
                </tr>
              </table></center>
              <p><strong>Table 2.</strong> Testing and Training score from the Base Model</p>
              <p>Our model demonstrated a strong testing score of 81.46% soley from our base model. In addition, the precision or the measure of reliability of predicting the median house value ranges was relatively high. In particular the precision of predicting the median house value greater than $400,000 was 78%. Whereas 60% precision was reported predicting a median house value between $250,000-400,000. Lastly, 85% precision was seen predicting a median house value of less than $250,000. Our model demonstrated strong predictive performance at the higher and lower end of the median house value range, however, showed acceptable precision predicing mid range values. This validates our observation shown in the Longitude and Latitude plots in the Data section where the median house value in the mid-range was more spread out across the state of California.</p>
              <!-- next segment -->

              <p><strong>Model Optimizations</strong></p>
              <p>Given the high Testing Data Score of 81.46% from our base model, it is safe to conclude our model would do a great job generalizing the findings for the new input data. However, we wanted to push the envelope in improving our models while balancing the fitting of our model. Two model optimization techniques were deployed in an attempt to improve the testing score from our current standing.
              </p>
              
              <p><strong>1. Random Forest Classification</strong></p>
              <center><table>
              <th>Scores</th>
              <th>Random Forest Classification (max_depth = 10)</th>
                <th>(max_depth = 5)</th>
                <th>Default</th>
            
                <tr>
                  <td><center>Training Score</center></td>
                  <td>0.9106994955199157</td>
                  <td>0.8138694375423537</td>
                  <td>1.0</td>
                  
                </tr>
                <tr>
                  <td><center>Testing Score</center></td>
                  <td>0.8573825503355704</td>
                  <td>0.8048098434004475</td>
                  <td>0.8726230425055929</td>
                  
                </tr>
                
                <tr>
                  <td><center>Delta</center></td>
                  <td>0.0533169451843450</td>
                  <td>0.0090595941419059</td>
                  <td>0.1273769574944080</td>
                  
                </tr>

                </table></center>
                  
              <p></p>
          
            
              <p></p>
              <p>In an effort to improve the predictive accuracy and our testing score, we have deployed the Random Forest Classifier. As shown in the table above, our model saw a higher testing score of 85.74%, however, it began to overfit as the depth of our model increased with a discrepancy of training and testing score of 5.33% which is still an acceptable difference in the real world. At Default, the trees perfectly predicts all of the train data, however, it fails to generalize the findings for new data. 
              </p>

              <p><strong>2. Feature Selection with Random Forests</strong></p>

              <center><table>
                <th>Scores</th>
                <th>Random Forest Classification (max_depth = 10)</th>
                  <th>(max_depth = 5)</th>
                  <th>Default</th>
              
                  <tr>
                    <td><center>Training Score</center></td>
                    <td>0.9030931871574002</td>
                    <td>0.8179979117723832</td>
                    <td>1.0</td>
                    
                  </tr>
                  <tr>
                    <td><center>Testing Score</center></td>
                    <td>0.8535917009199452</td>
                    <td>0.816793893129771</td>
                    <td>0.8651399491094147</td>
                    
                  </tr>
                  
                  <tr>
                    <td><center>Delta</center></td>
                    <td>0.0495014862374550</td>
                    <td>0.0012040186426119</td>
                    <td>0.1348600508905860</td>
                    
                  </tr>
  
                  </table></center>
                  <p><strong>Table 3.</strong> Feature Selection with Random Forests</p>
                  <p></p>
                  <p>Second approach to model optimization was to find feature importance or coefficients of our model in order to remove noise or features with less significance. We can see a slight decrease in the discrepancy between the score of the training set and that of the testing set</p>

                  <p></p>
                  
                  <center><img src="../Images/Log_Reg_Feature_Importance.PNG" alt="LR image1" height="400" width="550"></center>
                  <p><strong>Figure 2.</strong> Plot demonstrating feature importances</p>
                  <p>Feature Selection technique was used to remove the noise, irrelevant or collinear features. With collinear features, the regression models can become confused and the coefficients can vary from their true values. Random Forest Classification was used to score our model's test performance. Looking at the testing score with a max depth of 10 across the two model optimization techniques, we see a noticeable decrease in the difference between training score and testing score. By further reducing the dimensions of our input data, we were able to reduce the gap between the training score and the testing score by Random Forest Classification.</p>
                  
                  
                  
                
        

          
                  
                  <p></p>

              <p><strong>K-Nearest Neighbor</strong></p>
                  <p>Next technique we have used was KNN. We set the k-value parameter to tell the algorithm to ﬁnd the k-number of closest known data points. 
                    Then, the algorithm determines what the majority of surrounding data points are classiﬁed to determine the class of the new data point. 
                    We determined the best k value to be 11 in KNN to predict the house value classification
                  </p>

                  <center><img src="../Images/KNN_plot.PNG" alt="LR image1" height="400" width="550"></center>

                  <p></p>
                  <p>We notice the test accuracy begins to stabilize at k value of 11. As shown in the Figure above, performance of the test score progressively improved and started to plateau starting at k value of 11.
                    Euclidean distance through multiple dimensions. </p>
                  
                  <p></p>
                  <center><table>
                    <th colspan="2">KNN Prediction</th>
                    <tr>
                      <td><center>True</center></td>
                      <td>5849</td>
                    </tr>
                    <tr>
                      <td><center>False</center></td>
                      <td>1303</td>
                    </tr>
                  </table></center>
                  <p></p>
                  <center><img src="../Images/KNN_pred_vs_act.PNG" alt="LR image1" height="250" width="200"></center>
                  <p>Counts of True and False </p>
                  <center><img src="../Images/KNN_classification_report.PNG" alt="LR image1" height="200" width="600"></center>

        </div>
    </div>
      </div>
      </div>
      <script>
        $(document).ready(function(){
          $("#selector").change(function(){
            $("body").toggleClass("bg-secondary");
            $(".custom-control-label").toggleClass("text-white");
            $("h1").toggleClass("text-white");
            $("p").toggleClass("text-white");
            $(".jumbotron").toggleClass("bg-dark");
          });
        });
      </script>
</body>
</html>