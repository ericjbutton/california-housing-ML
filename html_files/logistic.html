<!DOCTYPE html>
<html lang="en-us">

<head>
  <link rel="icon" href="../Images/House_icon.png"
  <meta charset="UTF-8">
  <title>California Housing Machine Learning</title>

 <!-- Bring in our bootstrap stylesheet -->

 
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

 <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
 <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
 <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

 <link rel="stylesheet" href="style.css">
</head>

<body>
  <nav class="navbar navbar-expand-lg navbar-dark">
    <img src="../Images/california-flag-acegif-1.gif" height="45" width="55">
      <a class="navbar-brand" href="../index.html">ML California Housing</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarColor01" aria-controls="navbarColor01" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>      

      <div class="collapse navbar-collapse" id="navbarColor01">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item active">
            <a class="nav-link" href="../index.html"> Home <span class="sr-only">(current)</span></a>
          </li>
          <li class="nav-item">
              <a class="nav-link" href="data.html">Data</a>
            </li>
          <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                ML Models
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                <a class="dropdown-item" href="logistic.html">Logistic Regression </a>
                <a class="dropdown-item" href="linear_regression.html">Linear Regression</a>
                
              </div>
            </li>
            <div class="custom-control custom-switch mr-5">
              <input type="checkbox" class="custom-control-input" id="selector">
              <label class="custom-control-label" for="selector">Dark Mode</label>
            </div>
        </ul>
        </div>
      </nav>
<div class="container">


  
      <!-- Row 1 -->
      <div class="row">
        <div class="col-12 col-sm-12 col-md-12">
            <div class="jumbotron">
              <h1><strong>Logistic Regression Algorithm</strong></h1>
              <center><img src="../Images/LogReg_1.png" alt="LR image1" height="300" width="400"></center>

              <p> For our Project, we have decided to use Multinomial logistic regression where we have assigned three classifications to the Median Housing Value, categorizing into three different ranges as shown in Table 1 below.</p>
              <center><table>
                <p></p>  
                <th>Classification</th>
                <th>Bins</th>
                <tr>
                  <td><center>0</center></td>
                  <td>Median House Value greater than $400,000</td>
                </tr>
                <tr>
                  <td><center>1</center></td>
                  <td>Median House Value between $250,000 and $400,000</td>
                </tr>
                <tr>
                    <td><center>2</center></td>
                    <td>Median House Value less than $250,000</td>
                  </tr>
                
              </table></center>
              <p><strong>Table 1.</strong> Classifications of Median Housing Value in California. From the 1990 Census dataset.</p>
              <p></p>
              <p>With the deployment of the Logistic Regression, we were able to not only predict the range of the median house value by looking at the Housing dimensions but also attain an accuracy of the prediction of each range of the value and assess at what ranges our model had the strongest performance in predicting the median house value. </p>

              <p><strong>Classification Report of logistic regression</strong></p>
              <p> </p>
              
              <center><img src="../Images/Log_Reg_training_testing_score.PNG" alt="LR image1" height="300" width="600"></center>
              </p>
              <center><table>
                <th colspan="2">Base Model</th>
                <tr>
                  <td><center>Training Score</center></td>
                  <td>0.8091258188389429</td>
                </tr>
                <tr>
                  <td><center>Testing Score</center></td>
                  <td>0.8145973154362416</td>
                </tr>
              </table></center>
              <p><strong>Table 2.</strong> Testing and Training score from the Base Model</p>
              <p>Our model had a precision of 78% predicting the median house value greater than $400,000. Whereas 60% precision was reported predicting a median house value between $250,000-400,000. Lastly, 85% precision was seen predicting a median house value of less than $250,000. Our model demonstrated strong performance predicting a house value at the higher and lower end of the range, however, showed acceptable precision predicing mid range values. This validates our observation shown in the Longitude and Latitude plots in the Data section where the median house value in the mid-range was more spread out across the state of California.</p>
              <!-- next segment -->

              <p><strong>Model Optimizations</strong></p>
              <p>Given the high Testing Data Score of 81.46% from our base model, it is safe to conclude our model would do a great job generalizing the findings for the new input data. However, we wanted to push the envelope in improving our models while balancing the fitting of our model. Three model optimization techniques were deployed in an attempt to improve the testing score from our current standing.
              </p>
              
              <p><strong>1. Random Forest Classification</strong></p>
              <center><table>
              <th>Scores</th>
              <th>Random Forest Classification (max_depth = 10)</th>
                <th>(max_depth = 5)</th>
                <th>Default</th>
            
                <tr>
                  <td><center>Training Score</center></td>
                  <td>0.9106994955199157</td>
                  <td>0.8138694375423537</td>
                  <td>1.0</td>
                  
                </tr>
                <tr>
                  <td><center>Testing Score</center></td>
                  <td>0.8573825503355704</td>
                  <td>0.8048098434004475</td>
                  <td>0.8726230425055929</td>
                  
                </tr>
                
                <tr>
                  <td><center>Delta</center></td>
                  <td>0.0533169451843450</td>
                  <td>0.0090595941419059</td>
                  <td>0.1273769574944080</td>
                  
                </tr>

                </table></center>
                  
              <p></p>
          
            
              <p></p>
              <p>In an effort to improve the predictive accuracy and our testing score, we have deployed the Random Forest Classifier. As shown in the table above, our model saw a higher testing score of 85.74%, however, it began to overfit as the depth of our model increased with a discrepancy of training and testing score of 5.33% which is still an acceptable difference in the real world. At Default, the trees perfectly predicts all of the train data, however, it fails to generalize the findings for new data. 
              </p>

              <p><strong>2. Feature Selection with Random Forests</strong></p>

              <center><table>
                <th>Scores</th>
                <th>Random Forest Classification (max_depth = 10)</th>
                  <th>(max_depth = 5)</th>
                  <th>Default</th>
              
                  <tr>
                    <td><center>Training Score</center></td>
                    <td>0.9030931871574002</td>
                    <td>0.8179979117723832</td>
                    <td>1.0</td>
                    
                  </tr>
                  <tr>
                    <td><center>Testing Score</center></td>
                    <td>0.8535917009199452</td>
                    <td>0.816793893129771</td>
                    <td>0.8651399491094147</td>
                    
                  </tr>
                  
                  <tr>
                    <td><center>Delta</center></td>
                    <td>0.0495014862374550</td>
                    <td>0.0012040186426119</td>
                    <td>0.1348600508905860</td>
                    
                  </tr>
  
                  </table></center>
                  <p></p>
                  <p>Second approach to model optimization was to find feature importance or coefficients of our model in order to remove noise or features with less significance. We can see a slight decrease in the discrepancy between the score of the training set and that of the testing set</p>

                  <p></p>
                  <p><strong>Case study-training our models with collinear input features and cleaned input features</strong></p>
                  <center><img src="../Images/Log_Reg_Feature_Importance.PNG" alt="LR image1" height="400" width="550"></center>
                  
                  <p>Plot demonstrating different feature importances. Features including the collinear variables were intentionally selected to showcase that the models can become inefficient by an overabundance of features, creating an unwanted ﬁt to the noise of irrelevant features. In this Plot, correlated features are given equal or similar importance, but overall reduced importance compared to the same tree built without correlated counterparts.</p>
                  <center><img src="../Images/arrow.PNG" alt="LR image1" height=100" width="100"></center>
                  
                  <center><img src="../Images/Log_Reg_Feature_Importance2.PNG" alt="LR image1" height="400" width="550"></center>
                  <p>Plot demonstrating feature importances after removing collinear attributes.</p>
                  <p></p>

          

                <p></p>
              <p><strong> 3. PCA with Random Forest Classification</strong></p> 
              
              <center><table>
                <th>Scores</th>
                <th>PCA Random Forest Classification (max_depth = 10)</th>
                  <th>(max_depth = 5)</th>
                  <th>Default</th>
              
                  <tr>
                    <td><center>Training Score</center></td>
                    <td>0.854868180631688</td>
                    <td>0.799334377447141</td>
                    <td>1.0</td>
                    
                  </tr>
                  <tr>
                    <td><center>Testing Score</center></td>
                    <td>0.816793893129771</td>
                    <td>0.797024858093560</td>
                    <td>0.809356038363672</td>
                    
                  </tr>
                  
                  <tr>
                    <td><center>Delta</center></td>
                    <td>0.038074287501917</td>
                    <td>0.00230951935358092</td>
                    <td>0.190643961636328</td>
                    
                  </tr>
  
                  </table></center>
                  <p><strong>Table 5.</strong> PCA with Random Forest Classification.</p>
                

                  <p></p>
                  <p>With collinear features, the regression models can become confused and the coefficients can vary from their true values. we have taken a third approach to model optimization to reduce the dimension of the data by breaking down the input features into a number of independent factors primarily to remove collinearity issues. After the PCA, Random Forest Classification was used to score our model's test performance. Looking at the testing score with a max depth of 10 across the three model optimization techniques, we see a noticeable decrease in the difference between training score and testing score. By further reducing the dimensions of our input data, we were able to reduce the gap between the training score and the testing score.</p>
                  
                  <p></p>

              <p><strong>K-Nearest Neighbor</strong></p>
                  <p>Next technique we have used was KNN. We set the k-value parameter to tell the algorithm to ﬁnd the k-number of closest known data points. 
                    Then, the algorithm determines what the majority of surrounding data points are classiﬁed to determine the class of the new data point. 
                    We determined the best k value to be 11 in KNN to predict the house value classification
                  </p>

                  <center><img src="../Images/KNN_plot.PNG" alt="LR image1" height="400" width="550"></center>

                  <p></p>
                  <p>We notice the test accuracy begins to stabilize at k value of 11. As shown in the Figure above, performance of the test score progressively improved and started to plateau starting at k value of 11.
                    Euclidean distance through multiple dimensions. </p>
                  
                  <p></p>
                  <center><table>
                    <th colspan="2">KNN Prediction</th>
                    <tr>
                      <td><center>True</center></td>
                      <td>5849</td>
                    </tr>
                    <tr>
                      <td><center>False</center></td>
                      <td>1303</td>
                    </tr>
                  </table></center>
                  <p></p>
                  <center><img src="../Images/KNN_pred_vs_act.PNG" alt="LR image1" height="250" width="200"></center>
                  <p>Counts of True and False </p>
                  <center><img src="../Images/KNN_classification_report.PNG" alt="LR image1" height="200" width="600"></center>

        </div>
    </div>
      </div>
      </div>
      <script>
        $(document).ready(function(){
          $("#selector").change(function(){
            $("body").toggleClass("bg-secondary");
            $(".custom-control-label").toggleClass("text-white");
            $("h1").toggleClass("text-white");
            $("p").toggleClass("text-white");
            $(".jumbotron").toggleClass("bg-dark");
          });
        });
      </script>
</body>
</html>