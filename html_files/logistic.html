<!DOCTYPE html>
<html lang="en-us">

<head>
  <link rel="icon" href="../Images/House_icon.png"
  <meta charset="UTF-8">
  <title>California Housing Machine Learning</title>

 <!-- Bring in our bootstrap stylesheet -->

 
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

 <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
 <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
 <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

 <link rel="stylesheet" href="style.css">
</head>

<body>
  <nav class="navbar fixed-top navbar-expand-lg navbar-dark">
    <img src="../Images/california-flag-acegif-1.gif" height="45" width="55">
      <a class="navbar-brand" href="../index.html">ML California Housing</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarColor01" aria-controls="navbarColor01" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>      

      <div class="collapse navbar-collapse" id="navbarColor01">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item active">
            <a class="nav-link" href="../index.html"> Home <span class="sr-only">(current)</span></a>
          </li>
          <li class="nav-item">
              <a class="nav-link" href="data.html">Data</a>
            </li>
          <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                ML Models
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                <a class="dropdown-item" href="logistic.html">Logistic Regression </a>
                <a class="dropdown-item" href="linear_regression.html">Linear Regression</a>
                
              </div>
            </li>
            <div class="custom-control custom-switch mr-5">
              <input type="checkbox" class="custom-control-input" id="selector">
              <label class="custom-control-label" for="selector">Dark Mode</label>
            </div>
        </ul>
        </div>
      </nav>
<div class="container">


  
      <!-- Row 1 -->
      <div class="row">
        <div class="col-12 col-sm-12 col-md-12">
            <div class="jumbotron">
              <h1><strong>Logistic Regression Algorithm</strong></h1>
              

              <p> One of the regression models we have decided to use for our Project was Multinomial logistic regression where we have assigned three classifications to the Median Housing Value, categorizing the outputs into three different ranges as shown in Table 1 below.</p>
              <center><table>
                <p></p>  
                <th>Classification</th>
                <th>Bins</th>
                <tr>
                  <td><center>0</center></td>
                  <td>Median House Value greater than $400,000</td>
                </tr>
                <tr>
                  <td><center>1</center></td>
                  <td>Median House Value between $250,000 and $400,000</td>
                </tr>
                <tr>
                    <td><center>2</center></td>
                    <td>Median House Value less than $250,000</td>
                  </tr>
                
              </table></center>
              <p><strong>Table 1.</strong> Classifications of Median Housing Value in California. From the 1990 Census dataset.</p>
              <p></p>
              <p>With the deployment of the Logistic Regression, we were able to not only predict the range of the median house value by looking at different Housing dimensions but also attain a precision of predictions of each range of the value and assess at what ranges our model had the best precision in predicting the median house value. </p>

              <p><strong>Classification Report of logistic regression</strong></p>
              <p> </p>
              
              <center><img src="../Images/Log_Reg_training_testing_score.PNG" alt="LR image1" height="300" width="600"></center>
              </p>
              <center><table>
                <th colspan="2">Base Model</th>
                <tr>
                  <td><center>Training Score</center></td>
                  <td>0.8091258188389429</td>
                </tr>
                <tr>
                  <td><center>Testing Score</center></td>
                  <td>0.8145973154362416</td>
                </tr>
              </table></center>
              <p><strong>Table 2.</strong> Testing and Training score from the Base Model</p>
              <p>Our model demonstrated a strong testing score of 81.46% soley from our base model. In addition, the precision or the measure of reliability of predicting the median house value ranges was relatively high. In particular the precision of predicting the median house value greater than $400,000 was 78%. Whereas 60% precision was reported predicting a median house value between $250,000-400,000. Lastly, 85% precision was seen predicting a median house value of less than $250,000. Our model demonstrated strong predictive performance at the higher and lower end of the median house value range, however, showed acceptable precision predicing mid range values. This validates our observation shown in the Longitude and Latitude plots in the Data section where the median house value in the mid-range was more spread out across the state of California.</p>
              <!-- next segment -->

              <p><strong>Model Optimizations</strong></p>
              <p>Given the high Testing Data Score of 81.46% from our base model, it is safe to conclude our model would do a great job generalizing the findings for the new input data. However, we wanted to push the envelope in improving our models while balancing the fitting of our model. Two model optimization techniques were deployed in an attempt to improve the testing score from our current standing.
              </p>
              
              <p><strong>1. Random Forest Classification</strong></p>
              <center><table>
              <th>Scores</th>
              <th>Random Forest Classification (max_depth = 10)</th>
                <th>(max_depth = 5)</th>
                <th>Default</th>
            
                <tr>
                  <td><center>Training Score</center></td>
                  <td>0.9106994955199157</td>
                  <td>0.8138694375423537</td>
                  <td>1.0</td>
                  
                </tr>
                <tr>
                  <td><center>Testing Score</center></td>
                  <td>0.8573825503355704</td>
                  <td>0.8048098434004475</td>
                  <td>0.8726230425055929</td>
                  
                </tr>
                
                <tr>
                  <td><center>Delta</center></td>
                  <td>0.0533169451843450</td>
                  <td>0.0090595941419059</td>
                  <td>0.1273769574944080</td>
                  
                </tr>

                </table></center>
                  
              <p></p>
              <center><img src="../Images/Log_Reg_Random_Forest_score_plot.PNG" alt="LR image1" height="300" width="550"></center>
              <p><strong>Figure 2.</strong> Random Forest Score plot by varying max_depth</p>
              <p></p>
              <p>In an effort to improve the predictive accuracy and our testing score, we have deployed the Random Forest Regressor while tuning parameters to reduce model complexity (i.e. max_depth, tree depth, etc.). As shown in the table above, our model saw a higher testing score of 85.74%, an improvement from our base model, however, we quickly realized our model began to overfit as the depth of our model increased demonstrating an increasing discrepancy of training and testing score. At Default, the trees perfectly predicts all of the train data, however, it fails to generalize the findings for new data. Random forests have many degrees of freedom, so the model can achieve a 100% accuracy of training score or in-sample relatively fast. This leads to an overfitting problem.
              </p>

              <p><strong>2. Feature Selection with Random Forests</strong></p>

              <center><table>
                <th>Scores</th>
                <th>Random Forest Classification (max_depth = 10)</th>
                  <th>(max_depth = 5)</th>
                  <th>Default</th>
              
                  <tr>
                    <td><center>Training Score</center></td>
                    <td>0.9030931871574002</td>
                    <td>0.8179979117723832</td>
                    <td>1.0</td>
                    
                  </tr>
                  <tr>
                    <td><center>Testing Score</center></td>
                    <td>0.8535917009199452</td>
                    <td>0.816793893129771</td>
                    <td>0.8651399491094147</td>
                    
                  </tr>
                  
                  <tr>
                    <td><center>Delta</center></td>
                    <td>0.0495014862374550</td>
                    <td>0.0012040186426119</td>
                    <td>0.1348600508905860</td>
                    
                  </tr>
  
                  </table></center>
                  <p><strong>Table 3.</strong> Feature Selection with Random Forests</p>
                  <p></p>
                  <p>Second approach to model optimization was to further close the gap between the training set and testing set by identifying feature importance or coefficients of our model and subsequently removing noise or features with less significance such as redundant and/or collinear variables. We can see a slight decrease in the discrepancy between the score of the training set and that of the testing set</p>

                  <p></p>
                  
                  <center><img src="../Images/Log_Reg_Feature_Importance.PNG" alt="LR image1" height="400" width="550"></center>
                  <p><strong>Figure 3.</strong> Plot demonstrating feature importances</p>
                  <p>The Feature Selection technique was used to remove the potential noise, irrelevant or collinear features down to 5 attributes. Random Forest Classification was used to score our model's test performance. Looking at the testing score with a max depth of 10 across the two model optimization techniques, we see a noticeable decrease in the difference between training score and testing score. By further reducing the dimensions of our input data, we were able to reduce the gap between the training score and the testing score by Random Forest Classification.</p>
                  
                  
                  
                
        

          
                  
                  <p></p>

              <p><strong>K-Nearest Neighbor</strong></p>
                  <p>Next technique we have used was KNN. We set the k-value parameter to tell the algorithm to ﬁnd the k-number of closest known data points. 
                    Then, the algorithm determines what the majority of surrounding data points are classiﬁed to determine the class of the new data point. 
                    We determined the best k value to be 11 in KNN to predict the house value classification
                  </p>

                  <center><img src="../Images/KNN_plot.PNG" alt="LR image1" height="400" width="550"></center>

                  <p></p>
                  <p>We notice the test accuracy begins to stabilize at k value of 11. As shown in the Figure above, performance of the test score progressively improved and started to plateau starting at k value of 11.
                    Euclidean distance through multiple dimensions. </p>
                  
                  <p></p>
                  <center><table>
                    <th colspan="2">KNN Prediction</th>
                    <tr>
                      <td><center>True</center></td>
                      <td>5849</td>
                    </tr>
                    <tr>
                      <td><center>False</center></td>
                      <td>1303</td>
                    </tr>
                  </table></center>
                  <p></p>
                  <center><img src="../Images/KNN_pred_vs_act.PNG" alt="LR image1" height="250" width="200"></center>
                  <p>Counts of True and False </p>
                  <center><img src="../Images/KNN_classification_report.PNG" alt="LR image1" height="200" width="600"></center>

        </div>
    </div>
      </div>
      </div>
      <script>
        $(document).ready(function(){
          $("#selector").change(function(){
            $("body").toggleClass("bg-secondary");
            $(".custom-control-label").toggleClass("text-white");
            $("h1").toggleClass("text-white");
            $("p").toggleClass("text-white");
            $(".jumbotron").toggleClass("bg-dark");
          });
        });
      </script>
</body>
</html>